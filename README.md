# LLM Evaluation Framework with PromptFoo

## Overview
A comprehensive evaluation system to test multiple Large Language Models (LLMs) on generating accurate GitHub API curl commands using automated testing and quality assessment.

## Features
- Multi-provider LLM testing (OpenAI GPT-4o-mini, GPT-4.1-nano, Google Gemini)
- Automated assertion validation for API command accuracy
- LLM-based rubrics for qualitative assessment
- Configurable testing pipeline with caching
- AIPipe integration for API access

## Technologies Used
- **PromptFoo** - LLM evaluation framework
- **YAML** - Configuration management
- **Node.js** - Runtime environment
- **AIPipe** - LLM API integration

## Setup Instructions

### Prerequisites
- Node.js (v14 or higher)
- npm package manager
- AIPipe API token

### Installation
1. Clone the repository:
